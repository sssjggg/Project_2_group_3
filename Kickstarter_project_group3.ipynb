{
 "cells": [
  {
   "source": [
    "## 0.1 Import libarys"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "signed-pressing",
   "metadata": {},
   "outputs": [],
   "source": [
    "# general\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from pandas import to_datetime\n",
    "\n",
    "\n",
    "# plot libarys\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Model preperation\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.model_selection import train_test_split, RandomizedSearchCV, GridSearchCV, cross_validate\n",
    "from sklearn.preprocessing import OneHotEncoder, MinMaxScaler, StandardScaler, LabelEncoder, OrdinalEncoder\n",
    "from sklearn.compose import ColumnTransformer, make_column_selector\n",
    " \n",
    "\n",
    "# Classifier\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.tree import DecisionTreeClassifier, export_graphviz  \n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.ensemble import RandomForestClassifier, ExtraTreesClassifier\n",
    "from xgboost import XGBClassifier\n",
    "\n",
    "# Model Metrics\n",
    "from sklearn.metrics import precision_score, recall_score, roc_auc_score, roc_curve, confusion_matrix, classification_report, plot_confusion_matrix\n",
    "\n",
    "# for merging the dataframes\n",
    "import os, glob\n",
    "import json\n",
    "\n",
    "# further libarys\n",
    "import itertools\n",
    "from sklearn.tree import export_graphviz\n",
    "\n",
    "%matplotlib inline"
   ]
  },
  {
   "source": [
    "## 0.2 Merging the data frames and loading the data frame"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#path = 'data/'\n",
    "#all_files = glob.glob(os.path.join(path, '*.csv'))\n",
    "\n",
    "#df_from_each_file = (pd.read_csv(f) for f in all_files)\n",
    "#df_merged   = pd.concat(df_from_each_file, ignore_index=True)\n",
    "#df_merged.to_csv( \"data/Kickstarter.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "organic-holder",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv('data/Kickstarter.csv', index_col = [0])"
   ]
  },
  {
   "source": [
    "# 1. Data Cleaning"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "source": [
    "The following columns were droped, becouse they hold no usefull and/or interesting data. The decision was based on a simple consideration of the data frame.\n",
    "\n",
    "For further information about the columns please see columns.md"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "coordinate-utilization",
   "metadata": {},
   "outputs": [],
   "source": [
    "out = ['urls','source_url','currency_symbol', 'currency_trailing_code', 'creator', 'location', 'slug', 'usd_type','photo', 'name', 'blurb', 'profile']\n",
    "df.drop(columns = out, inplace = True)"
   ]
  },
  {
   "source": [
    "Dropping the following columns, becouse they only hold 300 values or empty spots and the rest NaNs"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "allied-routine",
   "metadata": {},
   "outputs": [],
   "source": [
    "out = ['friends','is_backing','is_starred', 'permissions']\n",
    "df.drop(columns = out, inplace = True)"
   ]
  },
  {
   "source": [
    "Calculate the datetimes, given in seconds:"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "three-magazine",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.created_at = pd.to_datetime(df.created_at, unit = 's')\n",
    "df.launched_at = pd.to_datetime(df.launched_at, unit = 's')\n",
    "df.state_changed_at = pd.to_datetime(df.state_changed_at, unit = 's')\n",
    "df.deadline = pd.to_datetime(df.deadline, unit = 's')"
   ]
  },
  {
   "source": [
    "Define the categorical columns and transform theire type from object to category"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "active-rugby",
   "metadata": {},
   "outputs": [],
   "source": [
    "categorical = ['country', 'currency','current_currency', 'spotlight','staff_pick','state', 'disable_communication', 'is_starrable']\n",
    "df[categorical] = df[categorical].astype(\"category\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "radio-australian",
   "metadata": {},
   "source": [
    "# 2. Feature engineering"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "rental-confusion",
   "metadata": {},
   "source": [
    "### 2.1 Extract data in dictionary in category column into separate columns with leading `\"category_\"`.\n",
    "\n",
    "The category column contained a dictionary with various information. We extracted the the parent category, added this information to the dataframe and dropped the category column"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "breeding-merchandise",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = df.join(pd.DataFrame(df[\"category\"].apply(lambda x: json.loads(x)).to_list()).add_prefix(f\"category_\"))\n",
    "\n",
    "# drop unrelevant categories created by json and change objects to categorical type\n",
    "df.drop(columns=[\"category\"], inplace=True)\n",
    "category_out = [\"category_id\", \"category_color\", \"category_position\", \"category_urls\"]\n",
    "df.drop(columns=category_out, inplace=True)\n",
    "category_categorical = [\"category_parent_id\", \"category_name\", \"category_slug\"]\n",
    "df[category_categorical] = df[category_categorical].astype(\"category\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_cat = df.category_slug.str.title().str.split(\"/\", expand=True).rename(columns={0: \"parent_category_name\", 1: \"subcategory_name\"})\n",
    "df = df.join(df_cat)\n",
    "\n",
    "#df.pivot_table(index=[\"parent_category_name\"], columns=[\"state\"], values=[\"backers_count\", \"pledged_average\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['parent_category_name'].astype(\"category\");"
   ]
  },
  {
   "source": [
    "### 2.2 Analyse duplicates\n",
    "Id is a uniquely identifying id for each project on kickstarter. Therefore, we can check for duplicates based on their id."
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.id.value_counts()"
   ]
  },
  {
   "source": [
    "We have observations with duplicated ids. How many are there?"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "(df.id.value_counts() == 2).sum()"
   ]
  },
  {
   "source": [
    "How many real duplicates, i.e. completely identical rows, do we have?"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.duplicated().sum()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "impossible-extension",
   "metadata": {},
   "source": [
    "Duplicates do not give additional information, therefore remove them."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "prompt-child",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = df.drop_duplicates()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.info()"
   ]
  },
  {
   "source": [
    "### 2.3 Adding additional columns calculated with the original given data\n",
    "\n",
    "Projects can be launched for different time spans. We calculated the duration each project was online (based on launch date and deadline. Further, the set up of a project may take some time. We calculated the preparation time of each project based on the date the project was created and when it was eventually launched."
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "human-suggestion",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calulate the time between launched_at and deadline\n",
    "df['duration'] =  (df.deadline - df.launched_at).dt.days.astype('int')\n",
    "\n",
    "# Calculate the time between project creation (on kickstarter) and lounching it (days)\n",
    "df['prep_time'] =  (df.launched_at - df.created_at ).dt.days.astype('int')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.prep_time.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.prep_time.unique()"
   ]
  },
  {
   "source": [
    "### 2.4 Conversion of usd_goal and creating additional goal and pleadged related data columns\n",
    "\n",
    "The projects are not solely US-based. To be able to compare the various project goals we transformed the goal based on the given static USD rate. We also computed the ratio between the pledged amount and the number of backers for each project."
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# conversion of goal in USD with static_usd_rate\n",
    "df[\"usd_goal\"] = df.goal * df.static_usd_rate\n",
    "\n",
    "df[\"log_usd_goal\"] = np.log10(df.usd_goal)\n",
    "df[\"pledged_average\"] = df.usd_pledged / df.backers_count\n",
    "df[\"log_pledged_average\"] = np.log10(df.pledged_average)"
   ]
  },
  {
   "source": [
    "### 2.5 Preparing the time related data for visualization \n",
    "\n",
    "Subdevision launched, deadline, changed and created in hours (_H), days (_D), months (_M) and years (_Y)"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['launched_Y'] = df.launched_at.dt.year.astype('int')\n",
    "df['launched_M'] = df.launched_at.dt.month.astype('int')\n",
    "df['launched_D'] = df.launched_at.dt.day.astype('int')\n",
    "df['launched_H'] = df.launched_at.dt.hour.astype('int')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['deadline_Y'] = df.deadline.dt.year.astype('int')\n",
    "df['deadline_M'] = df.deadline.dt.month.astype('int')\n",
    "df['deadline_D'] = df.deadline.dt.day.astype('int')\n",
    "df['deadline_H'] = df.deadline.dt.hour.astype('int')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['changed_Y'] = df.state_changed_at.dt.year.astype('int')\n",
    "df['changed_M'] = df.state_changed_at.dt.month.astype('int')\n",
    "df['changed_D'] = df.state_changed_at.dt.day.astype('int')\n",
    "df['changed_H'] = df.state_changed_at.dt.hour.astype('int')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['created_Y'] = df.created_at.dt.year.astype('int')\n",
    "df['created_M'] = df.created_at.dt.month.astype('int')\n",
    "df['created_D'] = df.created_at.dt.day.astype('int')\n",
    "df['created_H'] = df.created_at.dt.hour.astype('int')"
   ]
  },
  {
   "source": [
    "Project were online between 1 to 93 days. We subdevided the preparation time into the following 'duration_bins: '1 day', '3 days', '1 week', '2 weeks', '1 month', '2 months' and '3 months'. Apart from '1 day', all other bins should be understood as \"as long as\"."
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def dur_kick(x):\n",
    "        if x == 1: return '1 day'\n",
    "        elif x == 2 or x <= 3 : return '3 days' \n",
    "        elif x == 4 or x <= 7 : return '1 week'\n",
    "        elif x == 8 or x <= 14 : return '2 weeks'\n",
    "        elif x == 15 or x <= 30 : return '1 month'\n",
    "        elif x == 31 or x <= 60 : return '2 months'\n",
    "        elif x == 61 or x <= 93 : return '3 months'\n",
    "\n",
    "df[\"duration_bins\"] = pd.Categorical(df.duration.apply(dur_kick), \n",
    "                ['1 day', '3 days', '1 week', '2 weeks', '1 month', '2 months','3 months'])"
   ]
  },
  {
   "source": [
    "The preparation of a project was rather different. Thus, we we created another bin 'prep_bins' ('1 day', '3 days', '1 week', '2 weeks', '1 month', '2 months','3 months', '6 months', '1 year' and '> 1 year'). Here, each given bin should be read as \"at least as long as\"."
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def prep(x):\n",
    "        if x <= 1: return '1 day'\n",
    "        elif x <= 3 : return '3 days' \n",
    "        elif x <= 7 : return '1 week'\n",
    "        elif x <= 14 : return '2 weeks'\n",
    "        elif x <= 30 : return '1 month'\n",
    "        elif x <= 60 : return '2 months'\n",
    "        elif x <= 90 : return '3 months'\n",
    "        elif x <= 180 : return '6 months'\n",
    "        elif x <= 360 : return '1 year'\n",
    "        else : return '> 1 year'\n",
    "\n",
    "df[\"prep_bins\"] = pd.Categorical(df.prep_time.apply(prep), \n",
    "                ['1 day', '3 days', '1 week', '2 weeks', '1 month', '2 months','3 months', '6 months', '1 year', '> 1 year'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#checking for NaNs\n",
    "df.prep_bins.isnull().sum()"
   ]
  },
  {
   "source": [
    "As we calculated the date for each project, we might as well assign weekdays for the launch day and the deadline."
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "wday = ['Monday', 'Tuesday', 'Wednesday', 'Thursday', 'Friday', 'Saturday', 'Sunday']\n",
    "\n",
    "def weekday(x): return wday[x]       "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['launch_day'] = df.launched_at.dt.to_period('D').dt.weekday\n",
    "df['launch_day'] = pd.Categorical(df.launch_day.apply(weekday), \n",
    "                ['Monday', 'Tuesday', 'Wednesday', 'Thursday', 'Friday', 'Saturday', 'Sunday'])  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['deadline_day'] = df.deadline.dt.to_period('D').dt.weekday\n",
    "df['deadline_day'] = pd.Categorical(df.deadline_day.apply(weekday), \n",
    "                ['Monday', 'Tuesday', 'Wednesday', 'Thursday', 'Friday', 'Saturday', 'Sunday'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['changed_day'] = df.state_changed_at.dt.to_period('D').dt.weekday\n",
    "df['changed_day'] = pd.Categorical(df.changed_day.apply(weekday), \n",
    "                ['Monday', 'Tuesday', 'Wednesday', 'Thursday', 'Friday', 'Saturday', 'Sunday'])"
   ]
  },
  {
   "source": [
    "# 4. EDA\n",
    "## 4.1. The influence of time related data on the success"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "source": [
    "### 4.1.1 Closer look at the lanuched related data"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# seting figuresize and lable size globally\n",
    "plt.rcParams['figure.figsize']=(12,8)\n",
    "plt.rcParams['font.size']=14\n",
    "\n",
    "sns.set_theme(palette = 'pastel', \n",
    "              font_scale=1.25)\n",
    "\n",
    "# setting the colors\n",
    "state = df.state.unique().tolist()\n",
    "state_colors=['#fa9fb5', '#7a0177', '#8c96c6', '#f768a1', '#fcc5c0']\n",
    "COLOR_STATE = dict(zip(state, state_colors))\n",
    "COLOR_TIME = '#084594'  # dark blue-ish\n",
    "COLOR_COUNTRY = '#6baed6'  # blue-ish\n",
    "COLOR_CATEGORY = '#2171b5' # different blue\n",
    "COLOR_SUCCESS = '#7a0177'  # dark purple\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.launched_Y.value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(df['launched_M'].groupby(df['launched_Y']).value_counts())"
   ]
  },
  {
   "source": [
    "Let's look at the overall number of projects per year. Since its start in 2009, Kickstarter has has increasing number of projects on its platform. The highest number was reached in 2015 with almost 38,000 projects - so far. The last years there have been a bit more than 27,000 projects. However, we do expect an uptick in projects as numbers have risen in 2018 again and in 2019 there are already more than 8,000 projects in the first three months of the year."
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualization of the number of projects started annually \n",
    "sns.countplot(x = df.launched_Y.sort_values(), color = COLOR_TIME).set(xlabel='Year', ylabel = 'Number of projects')\n",
    "\n",
    "#plt.savefig(\"images/projects_year.png\",  bbox_inches=\"tight\")\n",
    "plt.show()"
   ]
  },
  {
   "source": [
    "The success rate of projects was fairly high in the first years of Kickstarter (almost 80%). However, it dropped in 2014 below 50%. After another low in 2015, the success rate as increased slightly over the last years."
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualization of the project state (read: success rate) over the different years.\n",
    "ax = sns.histplot(x='launched_Y',\n",
    "                 hue= 'state',\n",
    "                 stat = 'probability',\n",
    "                 data=df,\n",
    "                 multiple=\"fill\",\n",
    "                 palette = COLOR_STATE\n",
    "                 )\n",
    "ylabels = ['{:,.0f}'.format(y) for y in ax.get_yticks()*100]\n",
    "ax.set_yticklabels(ylabels)\n",
    "\n",
    "ax.set(xlabel=\"Year\")\n",
    "ax.set(ylabel=\"Percent\")\n",
    "\n",
    "plt.legend(bbox_to_anchor=(1.05, 1), loc='upper left', borderaxespad=0., \n",
    "           labels =['live', 'successful', 'failed', 'canceled', 'suspended'], labelspacing=1.2)\n",
    "\n",
    "#plt.savefig(\"images/state_year.png\", bbox_inches=\"tight\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ax = sns.countplot(x = df.launched_D.sort_values(), color = COLOR_TIME)\n",
    "\n",
    "ax.set(xlabel='Day of the month (launch)', ylabel = 'Number of projects')\n",
    "\n",
    "ylabels = ['{:,.0f}'.format(y) for y in ax.get_yticks()]\n",
    "ax.set_yticklabels(ylabels)\n",
    "\n",
    "#plt.savefig(\"images/projects_month.png\" , bbox_inches=\"tight\")\n",
    "plt.show()"
   ]
  },
  {
   "source": [
    "As you can see above, projects were launched on all days of the months and fairly evenly distributed. The first, the 15th and the 31st do stick out. But it's more interesting to see whether the launching day does indeed impact the success of a project. However, when looking at the state of the projects (below) we can see that it does not seem to be important on which day of the week a project is launched."
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ax = sns.histplot(x='launch_day',\n",
    "                 hue= 'state',\n",
    "                 stat = 'probability',\n",
    "                 data=df,\n",
    "                 multiple=\"fill\",\n",
    "                 palette = COLOR_STATE\n",
    "                 )\n",
    "#ax.set_xticklabels(ax.get_xticklabels(),rotation=40)\n",
    "ax.set(xlabel=\"Weekday\")\n",
    "ax.set(ylabel=\"Percent\")\n",
    "\n",
    "ylabels = ['{:,.0f}'.format(y) for y in ax.get_yticks()*100]\n",
    "ax.set_yticklabels(ylabels)\n",
    "\n",
    "plt.legend(bbox_to_anchor=(1.05, 1), loc='upper left', borderaxespad=0., \n",
    "           labels =['live', 'successful', 'failed', 'canceled', 'suspended'], labelspacing=1.2)\n",
    "\n",
    "#plt.savefig(\"images/state_weekday.png\", bbox_inches=\"tight\")\n",
    "plt.show()"
   ]
  },
  {
   "source": [
    "### 4.1.2 Exploration of the influence of the project duration on kickstarter"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "source": [
    "When preparing a project is also interesting to know for how long your project should be online to be successful - and if duration has an effect on your success. The previous projects were mostly online for 1 month, followed by 3 months."
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualization of duration projects were online (count)\n",
    "ax = sns.countplot(x = df.duration_bins, color = COLOR_TIME)\n",
    "\n",
    "ax.set(xlabel='Duration between launch and deadline', ylabel = 'Number of projects')\n",
    "ylabels = ['{:,.0f}'.format(y) for y in ax.get_yticks()]\n",
    "ax.set_yticklabels(ylabels)\n",
    "\n",
    "#plt.savefig(\"images/duration_bin_counts.png\", bbox_inches=\"tight\")\n",
    "plt.show()"
   ]
  },
  {
   "source": [
    "So does it have an effect on the success?\n",
    "\n",
    "It depends. Projects that have been online for at least 2 weeks or at least 3 months were more successful than other projects online for shorter (or longer) time spans. Also projects that were online for up to 1 week were more likely to be suspended."
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualization of duration bins in relation to state of projects\n",
    "ax = sns.histplot(x='duration_bins',\n",
    "                 hue= 'state',\n",
    "                 stat = 'probability',\n",
    "                 data=df,\n",
    "                 multiple=\"fill\",\n",
    "                 palette = COLOR_STATE\n",
    "                 )\n",
    "ax.set(xlabel=\"Duration between launch and deadline\")\n",
    "ax.set(ylabel=\"Percent\")\n",
    "\n",
    "ylabels = ['{:,.0f}'.format(y) for y in ax.get_yticks()*100]\n",
    "ax.set_yticklabels(ylabels)\n",
    "\n",
    "plt.legend(bbox_to_anchor=(1.05, 1), loc='upper left', borderaxespad=0., \n",
    "           labels =['live', 'successful', 'failed', 'canceled', 'suspended'], labelspacing=1.2)\n",
    "\n",
    "#plt.savefig(\"images/duration_bin_state.png\", bbox_inches=\"tight\")\n",
    "plt.show()"
   ]
  },
  {
   "source": [
    "Looking at the asked goal of a project, we can see that the duration of \"3 months\" may be distorting our plot above. It is likley that these projects were not successful because they were online for a longer period but rather because they were asking for a lower goal (compared to the projects in the two months bin)."
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualization  of the goals in relation to the duration the projects were online \n",
    "ax = sns.scatterplot(x = df.duration_bins, y = df.usd_goal, color = COLOR_TIME)\n",
    "\n",
    "ax.set(xlabel=\"Duration between launch and deadline\")\n",
    "ax.set(ylabel=\"Goal ins US Dollar\")\n",
    "\n",
    "ylabels = ['{:,.0f}'.format(y) for y in ax.get_yticks()]\n",
    "ax.set_yticklabels(ylabels)\n",
    "\n",
    "#plt.savefig(\"images/duration_bin_goal.png\", bbox_inches=\"tight\")\n",
    "plt.show()"
   ]
  },
  {
   "source": [
    "### 4.1.3 Preperationtime\n",
    "How long did it take between projects being created and actually launched on Kickstarter, and how did this affect their success? "
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ax = sns.countplot(x = df.prep_bins, color = COLOR_TIME)\n",
    "\n",
    "ax.set(xlabel=\"Preparation time (duration between creation and launch)\")\n",
    "ax.set(ylabel=\"Number of projects\")\n",
    "\n",
    "ax.set_xticklabels(ax.get_xticklabels(),rotation=40)\n",
    "\n",
    "ylabels = ['{:,.0f}'.format(y) for y in ax.get_yticks()]\n",
    "ax.set_yticklabels(ylabels)\n",
    "\n",
    "#plt.savefig(\"images/preparation_bin_count.png\", bbox_inches=\"tight\")\n",
    "plt.show()"
   ]
  },
  {
   "source": [
    "A lot of the projects on kickstarter were created and put online within 24 hours. Another higher number of projects were prepared for up to one month. The numbers of project decline with a longer preparation time, but even projects that were worked on for more than a year were brought online eventually.\n",
    "\n",
    "So did this preparation time have an effect on the success (and maybe other possible states) of these projects?\n",
    "\n",
    "As you can see below projects that were prepared for one to three months were more likely to be successful. A longer preparation did not seem to have a positive effect on the outcome. And: projects that were launched very fast were more likely to be suspended."
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualization of state of a project in relation to preparation time\n",
    "ax = sns.histplot(x='prep_bins',\n",
    "                 hue= 'state',\n",
    "                 stat = 'probability',\n",
    "                 data=df,\n",
    "                 multiple=\"fill\",\n",
    "                 palette = COLOR_STATE\n",
    "                 )\n",
    "#ax.set_xticklabels(ax.get_xticklabels(),rotation=40)\n",
    "ax.set(xlabel=\"Duration between creation and launch\")\n",
    "ax.set(ylabel=\"Percent\")\n",
    "\n",
    "ylabels = ['{:,.0f}'.format(y) for y in ax.get_yticks()*100]\n",
    "ax.set_yticklabels(ylabels)\n",
    "\n",
    "plt.legend(bbox_to_anchor=(1.05, 1), loc='upper left', borderaxespad=0., \n",
    "           labels =['live', 'successful', 'failed', 'canceled', 'suspended'], labelspacing=1.2)\n",
    "\n",
    "#plt.savefig(\"images/prep_bin_state.png\", bbox_inches=\"tight\")\n",
    "plt.show()"
   ]
  },
  {
   "source": [
    "### 4.1.4 Deadline\n",
    "\n",
    "Let's take a closer look at the deadline. The data shows it doesn't seem to have an effect when a project is ending. The success rate seems fairly evenly distributed over the months of an year (in terms of deadline)."
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualization of the state of a project in relation to the month of the deadline\n",
    "ax = sns.histplot(x='deadline_M',\n",
    "                 hue= 'state',\n",
    "                 stat = 'probability',\n",
    "                 data=df,\n",
    "                 multiple=\"fill\",\n",
    "                 palette = COLOR_STATE\n",
    "                 )\n",
    "#ax.set_xticklabels(ax.get_xticklabels(),rotation=40)\n",
    "ax.set(xlabel=\"Month (in numbers)\")\n",
    "ax.set(ylabel=\"Percent\")\n",
    "\n",
    "ylabels = ['{:,.0f}'.format(y) for y in ax.get_yticks()*100]\n",
    "ax.set_yticklabels(ylabels)\n",
    "\n",
    "plt.legend(bbox_to_anchor=(1.05, 1), loc='upper left', borderaxespad=0., \n",
    "           labels =['live', 'successful', 'failed', 'canceled', 'suspended'], labelspacing=1.2)\n",
    "\n",
    "#plt.savefig(\"images/deadline_M_state.png\", bbox_inches=\"tight\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ax = sns.countplot(x = df.deadline_D.sort_values(), color = COLOR_TIME)\n",
    "\n",
    "ax.set(xlabel=\"Day of the month of the deadline\")\n",
    "ax.set(ylabel=\"Number of projects\")\n",
    "\n",
    "ylabels = ['{:,.0f}'.format(y) for y in ax.get_yticks()]\n",
    "ax.set_yticklabels(ylabels)\n",
    "\n",
    "#plt.savefig(\"images/deadline_D_count.png\", bbox_inches=\"tight\")\n",
    "plt.show()"
   ]
  },
  {
   "source": [
    "So maybe it has an effect on which days of the month the most deadlines were. In fact, the plot looks fairly similar to the plot of the launch day. The first, 15th/16th and the 30th of a month are popular deadlines. And just like the launch day the weekday does not seem to have an effect on the outcome of a project:"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualization of the state of a project in relation to the weekday of the deadline\n",
    "ax = sns.histplot(x='deadline_day',\n",
    "                 hue= 'state',\n",
    "                 stat = 'probability',\n",
    "                 data=df,\n",
    "                 multiple=\"fill\",\n",
    "                 palette = COLOR_STATE\n",
    "                 )\n",
    "#ax.set_xticklabels(ax.get_xticklabels(),rotation=40)\n",
    "ax.set(xlabel=\"Duration between creation and launch\")\n",
    "ax.set(ylabel=\"Percent\")\n",
    "\n",
    "ylabels = ['{:,.0f}'.format(y) for y in ax.get_yticks()*100]\n",
    "ax.set_yticklabels(ylabels)\n",
    "\n",
    "plt.legend(bbox_to_anchor=(1.05, 1), loc='upper left', borderaxespad=0., \n",
    "           labels =['live', 'successful', 'failed', 'canceled', 'suspended'], labelspacing=1.2)\n",
    "\n",
    "#plt.savefig(\"images/deadline_D_state.png\", bbox_inches=\"tight\")\n",
    "plt.show()"
   ]
  },
  {
   "source": [
    "## 4.2 Influence of the location on the success"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.groupby(df['country']).country.value_counts()"
   ]
  },
  {
   "source": [
    "The most projects are generated in the US, by far, which is not surprising as Kickstarter is based in Brooklyn, N.Y.C. Though, also a quite large number come from Great Britian and Canada."
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualization of project counts in relation to country \n",
    "ax = sns.countplot(x = df.country.sort_values(), color = COLOR_COUNTRY)\n",
    "\n",
    "ax.set(xlabel=\"Country\")\n",
    "ax.set(ylabel=\"Number of projects\")\n",
    "\n",
    "ylabels = ['{:,.0f}'.format(y) for y in ax.get_yticks()]\n",
    "ax.set_yticklabels(ylabels)\n",
    "\n",
    "#plt.savefig(\"images/country_count.png\", bbox_inches=\"tight\")\n",
    "plt.show()"
   ]
  },
  {
   "source": [
    "To check whether the country of were a project is based has an impact we created a column that displays the success rate of a project depending on the location. For this we ignore the other states a project might have - besides failed (canceled, suspended, live)."
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['success'] = df.state == 'successful'\n",
    "s_country = df.groupby(df['country']).success.mean().reset_index().rename(columns={\"success\":\"success_country\"})\n",
    "\n",
    "df = df.merge(s_country, how = 'outer', left_on = 'country', right_on = 'country')"
   ]
  },
  {
   "source": [
    "The most successful projects were launched in Hongkong and Luxemburg (above 60% success rate), Great Britian and Japan being the runners up. Remarkedly, project from Italy are rarely successful (below 30%)."
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualization of success rate in relation to location/country\n",
    "ax = sns.barplot(x = df.country, y = df.success_country, color = COLOR_SUCCESS)\n",
    "\n",
    "ax.set(xlabel=\"Country\")\n",
    "ax.set(ylabel=\"Success in percent\")\n",
    "\n",
    "ylabels = ['{:,.0f}'.format(y) for y in ax.get_yticks()*100]\n",
    "ax.set_yticklabels(ylabels)\n",
    "\n",
    "#plt.savefig(\"images/country_success.png\", bbox_inches=\"tight\")\n",
    "plt.show()"
   ]
  },
  {
   "source": [
    "## 4.3 Influence of the categorical dataccolumns "
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "source": [
    "Projects can belong to very different categories. The most popular ones - in terms of numbre of projects are \"Music\" and \" Film & Video\", followed by \"Publishing\", \"Art\" and \"Technology\"."
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualization of project counts in relation to category\n",
    "ax = sns.countplot(y = df.parent_category_name, color = COLOR_CATEGORY)\n",
    "\n",
    "ax.set(ylabel=\"\")\n",
    "ax.set(xlabel=\"Number of projects\")\n",
    "\n",
    "xlabels = ['{:,.0f}'.format(x) for x in ax.get_xticks()]\n",
    "ax.set_xticklabels(xlabels)\n",
    "\n",
    "#plt.savefig(\"images/category_count.png\", bbox_inches=\"tight\")\n",
    "plt.show()"
   ]
  },
  {
   "source": [
    "Are certain categories more successful than others? For the sake of simplicity of the next plot we also created a success rate column in relation for each category."
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "s_category = df.groupby(df['parent_category_name']).success.mean().reset_index().rename(columns={\"success\":\"success_category\"})\n",
    "df = df.merge(s_category, how = 'outer', left_on = 'parent_category_name', right_on = 'parent_category_name')\n",
    "s_category"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualization of success rate in relation to category\n",
    "ax = sns.barplot(y = df.parent_category_name, x = df.success_category, color = COLOR_SUCCESS)\n",
    "\n",
    "ax.set(ylabel=\"\")\n",
    "ax.set(xlabel=\"Success in percent\")\n",
    "\n",
    "xlabels = ['{:,.0f}'.format(x) for x in ax.get_xticks()*100]\n",
    "ax.set_xticklabels(xlabels)\n",
    "\n",
    "#plt.savefig(\"images/category_success.png\", bbox_inches=\"tight\")\n",
    "plt.show()"
   ]
  },
  {
   "source": [
    "THis plot looks very different to the above count of projects per category. The most successful projects are running the categories \"Comics\" and \"Dance\" (above 70%). In both categories the number of projects is fairly low compared to other categories. The least successful categories are Food, Journalism and Technology (below 35%).\n",
    "\n",
    "When looking a the number of backers we see something slightly different. The most backers are supporting projects in the category \"Games\" - by far! Next is Desgin, closely followed by Technology and Comics."
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ax = sns.barplot(y = df.parent_category_name, x= df.backers_count, color = COLOR_CATEGORY)\n",
    "\n",
    "ax.set(ylabel=\"\")\n",
    "ax.set(xlabel=\"Number of backers\")\n",
    "\n",
    "#plt.savefig(\"images/category_backers.png\", bbox_inches=\"tight\")\n",
    "plt.show()"
   ]
  },
  {
   "source": [
    "When looking at the averaged pledged amount per category, we can see that Technology projects are very popular. However, it does not result in a successful outcome - as seen above."
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ax = sns.barplot(y = df.parent_category_name, x= df.pledged_average, color = COLOR_CATEGORY)\n",
    "\n",
    "ax.set(ylabel=\"\")\n",
    "ax.set(xlabel=\"Ratio between pledged amount and goal (percent)\")\n",
    "\n",
    "#plt.savefig(\"images/category_pledged_av.png\", bbox_inches=\"tight\")\n",
    "plt.show()"
   ]
  },
  {
   "source": [
    "## 4.4 Staff pick"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "source": [
    "When a project is picked by kcikstarter staff is put in a certain spotlight and highlighted on the website. We would think this should have an effect on the success rate. Let's see."
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "s_pick = df.groupby(df['staff_pick']).success.mean().reset_index().rename(columns={\"success\":\"success_pick\"})\n",
    "df = df.merge(s_pick, how = 'outer', left_on = 'staff_pick', right_on = 'staff_pick')\n",
    "s_pick"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.groupby(df['staff_pick']).success.value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.groupby(df['staff_pick']).success.value_counts()"
   ]
  },
  {
   "source": [
    "Indeed! Just about 11% of all projects were staff picks. However these were than very successful - with a rate of about 87%. Projects that weren't picked showed a success rate of about 48%."
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots()\n",
    "labels = [' ', \n",
    "         'Staff pick']\n",
    "percentages = [89.3, 10.7]\n",
    "explode=(0.1,0)\n",
    "ax.pie(percentages, explode=explode, labels=labels, autopct='%1.0f%%', \n",
    "       shadow=False, startangle=0,  colors = ['#9ecae1', '#08306b'] ,\n",
    "       pctdistance=1.2,labeldistance=1.4)\n",
    "ax.axis('equal')\n",
    "\n",
    "#plt.savefig(\"images/staffpick.png\", bbox_inches=\"tight\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots()\n",
    "labels = ['Successful', \n",
    "         '']\n",
    "percentages = [86.7, 13.3]\n",
    "explode=(0.1,0)\n",
    "plt.title('Staff pick')\n",
    "ax.pie(percentages, explode=explode, labels=labels, autopct='%1.0f%%', \n",
    "       colors = ['#7a0177', '#8c96c6', ],\n",
    "       shadow=False, startangle=0,   \n",
    "       pctdistance=1.2,labeldistance=1.4)\n",
    "ax.axis('equal')\n",
    "\n",
    "#plt.savefig(\"images/staffpick_success.png\", bbox_inches=\"tight\")\n",
    "plt.show()"
   ]
  },
  {
   "source": [
    "Particularly projects from the categories \"Film & Video\" and \"Publishing\" are picked by Kickstarter staff, whereas projects in \"Crafts\" and \"Journalism\" are less likely to be picked."
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.groupby(df['staff_pick']).parent_category_name.value_counts()"
   ]
  },
  {
   "source": [
    "## 4.5 Disable communication\n",
    "\n",
    "Something we haven't looked at so far is a feature that seems to be minor: disable communication. Let's calculated the success rate ..."
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "s_comm = df.groupby(df['disable_communication']).success.mean().reset_index().rename(columns={\"success\":\"success_comm\"})\n",
    "df = df.merge(s_comm, how = 'outer', left_on = 'disable_communication', right_on = 'disable_communication')\n",
    "\n",
    "s_comm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.groupby(df['disable_communication']).state.value_counts()"
   ]
  },
  {
   "source": [
    "Above you can see that when communication is disabled there is no success at all. How come?\n",
    "\n",
    "When we look at the state of projects in relation to disabled communication we see that all projects where communication was disabled have been suspended by Kickstarter!"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "source": [
    "## 4. Projects that are \"starrable\"\n",
    "\n",
    "Another - maybe minor - feature is \"is_starrable\". What information does it hold, i.e. does it affect the success of a project?"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "s_star = df.groupby(df['is_starrable']).success.mean().reset_index().rename(columns={\"success\":\"success_star\"})\n",
    "df = df.merge(s_star, how = 'outer', left_on = 'is_starrable', right_on = 'is_starrable')\n",
    "\n",
    "s_star"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.groupby(df['is_starrable']).state.value_counts()"
   ]
  },
  {
   "source": [
    "All projects that are starrable are still live and we cannot assume the success by this feature."
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "source": [
    "# 5. Prepare data for model training \n",
    "## 5.1 Define target:"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "source": [
    "The aim of this project is to help potential project creators assess whether or not Kickstarter is a good funding option for them. Therefore, we want to model the chances to successfully raise enough money on Kickstarter. Kickstarter allwos backers to cancel their pledge and creators to cancel funding while the project is live. According to these cancellation policies, live or canceled projects could still miss the funding goal at the deadline although they had reached the funding goal earlier. Therefore, we only include successful, failed, and suspended projects in our analysis, treating failed and suspended both as not successful.\n",
    "\n",
    "Let's check the distribution of state classes before removing live and canceled data:"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Number of projects per state\n",
    "df.groupby(\"state\").backers_count.count()"
   ]
  },
  {
   "source": [
    "Most of the projects are recorded as failed or successful. What percentage of data would we drop by removing live and canceled projects?"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Percentage of live and canceled data\n",
    "df.query(\"state in ['live', 'canceled']\").shape[0]/df.shape[0] * 100"
   ]
  },
  {
   "source": [
    "We drop ~7.3% of the data.\n",
    "\n",
    "We generate a new target column with 1 for successful projects and 0 for failed and suspended projects."
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def target(row):\n",
    "    if row.state == \"successful\":\n",
    "        return 1\n",
    "    elif row.state in [\"failed\", \"suspended\"]:\n",
    "        return 0\n",
    "    else:\n",
    "        return np.nan"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create the new target column\n",
    "df[\"successful\"] = df.apply(lambda row: target(row), axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Drop live and canceled projects (they are the only rows with NaN values)\n",
    "df.dropna(axis=0, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## 5.2 Check for inbalance"
   ]
  },
  {
   "source": [
    "Let's check the class distribution of our target variable:"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.successful.value_counts() / df.shape[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f\"Number of projects with duplicate ID: {(df.id.value_counts() == 2).sum()}\") \n",
    "print(f\"Number of observations: {df.shape[0]}\")"
   ]
  },
  {
   "source": [
    "With a class distribution of 60% successful and 39% unsuccessful we have an almost balanced dataset"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "source": [
    "# 6. Modeling\n",
    "Although we found differences in success depending on the year of the launch, including it in the modeling could lead to overfitting to old economic situations. Information such as \"your project would have been successful in 2013\" is not relevant for our stakeholder who wants to realize their project now.\n",
    "\n",
    "We include features in our model, that are known/decided on project creation such as the funding goal in US Dollar, whether or not communication with the creator is enabled, the country of the project, the duration of the funding, the duration of project preparation, the name of the parent category, the name of the subcategory, and whether or not the project can be starred by users.\n",
    "\n",
    "As target we choose whether or not a project was successful, as perpared above."
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "source": [
    "## 6.1 Define target (X) and features (y)"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Select features\n",
    "features = [\"usd_goal\", \"disable_communication\", \"country\", \"duration\", \"prep_time\", \n",
    "            \"parent_category_name\", \"category_name\"]\n",
    "X = df[features]\n",
    "\n",
    "# Select target\n",
    "y = df.successful"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X.info()"
   ]
  },
  {
   "source": [
    "## 6.2 Spliting the data in train and test sets\n",
    "To be able to choose a model based on its performance on unseen data, we split our dataset into training and test set. We choose a random seed to have a reproducible split and no data leakage in our model selection process.\n",
    "\n",
    "The split was realized with sklearns train_test_split method, with a 70/30 ratio, random_state = 42 and stratify = target (the latter shall secure that the propration of vales in the training and test set have the same propration)"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Split the data in test and training set\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=42, stratify=y)"
   ]
  },
  {
   "source": [
    "## 6.3 Function definations\n",
    "For model evaluation, ................"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def pred_eval_plot_model(X_train, X_test, y_train, y_test, clf, cv=None):\n",
    "    \"\"\"Train a single model and print evaluation metrics.\n",
    "    \n",
    "    Args:\n",
    "        X_train (pd.DataFrame, np.array): Features of the training set\n",
    "        X_test (pd.DataFrame, np.array): Features of thee test set\n",
    "        y_train (pd.Series, np.array): Target of the training set\n",
    "        y_teset (pd.Seeries, np.array): Target of the test set\n",
    "        clf (sklearn.base.BaseEstimator): Estimator to train and use\n",
    "        cv (int, None): Number of cross-validations, default=None\n",
    "    \n",
    "    Returns:\n",
    "        model (sklearn.base.BaseEstimator): The trained model\n",
    "    \"\"\"\n",
    "    model = clf.fit(X_train, y_train)\n",
    "\n",
    "    if cv:\n",
    "        cv = cross_validate(m_rf, X_train_trans, y_train, cv=5, verbose=5)\n",
    "        print(f\"Best cross-validated score: {cv['test_score'].mean()}\")\n",
    "    \n",
    "    y_train_pred = model.predict(X_train)\n",
    "    y_pred = model.predict(X_test)\n",
    "    \n",
    "    print(f\"--- MODEL PARAMETERS {'-'*10}\")\n",
    "    print(json.dumps(model.get_params(), indent=4))\n",
    "    print(f\"--- CLASSIFICATION REPORT {'-'*10}\")\n",
    "    print(classification_report(y_test,y_pred))\n",
    "    print(f\"--- CONFUSION MATRIX {'-'*10}\")\n",
    "    print(confusion_matrix(y_test,y_pred))\n",
    "    plot_confusion_matrix(model, X_test, y_test)\n",
    "    return model\n",
    "\n",
    "def _pred_eval_plot_grid(X_train, X_test, y_train, y_test, gs):\n",
    "    \"\"\"Helper function to perform a grid search and calculate performance metrics.\n",
    "    \n",
    "    Args:\n",
    "        X_train (pd.DataFrame, np.array): Features of the training set\n",
    "        X_test (pd.DataFrame, np.array): Features of thee test set\n",
    "        y_train (pd.Series, np.array): Target of the training set\n",
    "        y_teset (pd.Seeries, np.array): Target of the test set\n",
    "        gs (BaseSearchCV): SearchCV to train and use\n",
    "    \n",
    "    Returns:\n",
    "        model (BaseSearchCV): The trained grid search\n",
    "    \"\"\"\n",
    "    gs = gs.fit(X_train, y_train)\n",
    "    \n",
    "    # Testing predictions (to determine performance)\n",
    "    y_pred = gs.best_estimator_.predict(X_test)\n",
    "    \n",
    "    print(f\"--- GRID SEARCH RESULTS {'-'*10}\")\n",
    "    print(f\"Best model: {gs.best_params_}\")\n",
    "    print(f\"Best cross-validated score: {gs.best_score_}\")\n",
    "    print(f\"--- CLASSIFICATION REPORT {'-'*10}\")\n",
    "    print(classification_report(y_test,y_pred))\n",
    "    print(f\"--- CONFUSION MATRIX {'-'*10}\")\n",
    "    print(confusion_matrix(y_test,y_pred))\n",
    "    plot_confusion_matrix(gs.best_estimator_, X_test, y_test)\n",
    "    return gs\n",
    "    \n",
    "\n",
    "def run_rand_grid_search(X_train, X_test, y_train, y_test, clf, params_grid, n_iter=10, cv=5):\n",
    "    \"\"\"Perform a randomized grid search and calculate performance metrics.\n",
    "    \n",
    "    Args:\n",
    "        X_train (pd.DataFrame, np.array): Features of the training set\n",
    "        X_test (pd.DataFrame, np.array): Features of thee test set\n",
    "        y_train (pd.Series, np.array): Target of the training set\n",
    "        y_teset (pd.Seeries, np.array): Target of the test set\n",
    "        clf (sklearn.base.BaseEstimator): Estimator to train and use\n",
    "        params_grid (dict): Dictionary defining the parameters for the grid search\n",
    "        n_iter (int): Number of grid search combinations to run\n",
    "        cv (int, None): Number of cross-validations, default=None\n",
    "        \n",
    "    Returns:\n",
    "        model (BaseSearchCV): The trained grid search\n",
    "    \"\"\"\n",
    "    gs = RandomizedSearchCV(clf, params_grid, n_iter=n_iter, cv=cv, random_state=24, verbose=5)\n",
    "    return _pred_eval_plot_grid(X_train, X_test, y_train, y_test, gs)\n",
    "    \n",
    "def run_grid_search(X_train, X_test, y_train, y_test, clf, params_grid, cv=5):\n",
    "    \"\"\"Perform a grid search and calculate performance metrics.\n",
    "    \n",
    "    Args:\n",
    "        X_train (pd.DataFrame, np.array): Features of the training set\n",
    "        X_test (pd.DataFrame, np.array): Features of thee test set\n",
    "        y_train (pd.Series, np.array): Target of the training set\n",
    "        y_teset (pd.Seeries, np.array): Target of the test set\n",
    "        clf (sklearn.base.BaseEstimator): Estimator to train and use\n",
    "        params_grid (dict): Dictionary defining the parameters for the grid search\n",
    "        cv (int, None): Number of cross-validations, default=None\n",
    "        \n",
    "    Returns:\n",
    "        model (BaseSearchCV): The trained grid search\n",
    "    \"\"\"\n",
    "    gs = GridSearchCV(clf, params_grid, cv=cv, verbose=5)\n",
    "    return _pred_eval_plot_grid(X_train, X_test, y_train, y_test, gs)\n",
    "    "
   ]
  },
  {
   "source": [
    "## 6.4 Preparation of Data Scaling and Category Encoding"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# encoder for categories\n",
    "onehot = OneHotEncoder(drop=\"first\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# scalers for numerical features\n",
    "mms = MinMaxScaler()\n",
    "ss = StandardScaler()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Prepare list of numerical and categorical columns\n",
    "num_cols = make_column_selector(dtype_include=np.number)\n",
    "cat_cols = make_column_selector(dtype_include=\"category\")"
   ]
  },
  {
   "source": [
    "# 7. Modeltesting"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "source": [
    "## 7.1 Logistic Regression\n",
    "Data Transformation\n",
    "For Logistic Regression we need to scale our data and encode categorical data. As the categories are not ordinal, we use one hot encoding."
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "source": [
    "### 7.1.1 Simple Logistic Regression with Standard Scaling"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define transformer\n",
    "transformer = ColumnTransformer([\n",
    "    (\"scale\", ss, num_cols),\n",
    "    (\"encode\", onehot, cat_cols),\n",
    "])\n",
    "\n",
    "# Transform\n",
    "X_train_trans = transformer.fit_transform(X_train)\n",
    "X_test_trans = transformer.transform(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "logreg_ss = LogisticRegression(max_iter=400)\n",
    "m_logreg_ss = pred_eval_plot_model(X_train_trans, X_test_trans, y_train, y_test, logreg_ss)"
   ]
  },
  {
   "source": [
    "The basic model with standard scaling of the numerical features achieves an accuracy of 75% and a precision of 78% on successful projects."
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Classification report\n",
    "y_probs = m_logreg_ss.predict_proba(X_test_trans)[:, 1]\n",
    "\n",
    "y_pred = y_probs > 0.9\n",
    "print(f\"--- CLASSIFICATION REPORT {'-'*10}\")\n",
    "print(classification_report(y_test,y_pred))\n",
    "print(f\"--- CONFUSION MATRIX {'-'*10}\")\n",
    "print(confusion_matrix(y_test,y_pred))"
   ]
  },
  {
   "source": [
    "Adjusting the threshold to 0.9, the precision for successful projects can be increased to 99%, reducing the accuracy to 65%."
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "source": [
    "### 7.1.2 Simple Logistic Regression with MinMax Scaling"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define transformer\n",
    "transformer = ColumnTransformer([\n",
    "    (\"scale\", mms, num_cols),\n",
    "    (\"encode\", onehot, cat_cols),\n",
    "])\n",
    "\n",
    "# Transform\n",
    "X_train_trans = transformer.fit_transform(X_train)\n",
    "X_test_trans = transformer.transform(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate the LogisticRegression\n",
    "logreg = LogisticRegression(max_iter=400)\n",
    "m_logreg_mm = pred_eval_plot_model(X_train_trans, X_test_trans, y_train, y_test, logreg)"
   ]
  },
  {
   "source": [
    "Using MinMax scaling, the model has an accuracy of 74% and precision of 79%."
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Classification report\n",
    "y_probs = m_logreg_mm.predict_proba(X_test_trans)[:, 1]\n",
    "\n",
    "y_pred = y_probs > 0.9\n",
    "print(f\"--- CLASSIFICATION REPORT {'-'*10}\")\n",
    "print(classification_report(y_test,y_pred))\n",
    "print(f\"--- CONFUSION MATRIX {'-'*10}\")\n",
    "print(confusion_matrix(y_test,y_pred))"
   ]
  },
  {
   "source": [
    "The same results can be achieved by adjusting the threshold value: Decrease in accuracy to 65% for an increase in precision to 99%.\n",
    "\n",
    "Hence, we can not say, that one scaling outperforms the other in case of logistic regression."
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "source": [
    "### 7.1.3 With Randomized Grid Search\n",
    "Let's try different regularization weights and types to improve the performance of the logistic regression:"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "params_grid = {\n",
    "    \"penalty\": [\"elasticnet\"],\n",
    "    \"C\": np.logspace(-3, 3, 7),\n",
    "    \"max_iter\": [200],\n",
    "    \"l1_ratio\": np.arange(0, 1, 0.25),\n",
    "    \"solver\": [\"saga\"],\n",
    "}\n",
    "rs_logreg = run_rand_grid_search(X_train_trans, X_test_trans, y_train, y_test, logreg, params_grid, cv=3, n_iter=20)"
   ]
  },
  {
   "source": [
    "The best model with randomized search is achieved with {'solver': 'saga', 'penalty': 'elasticnet', 'max_iter': 200, 'l1_ratio': 0.25, 'class_weight': None, 'C': 1000.0} With a cross-validated score of 0.7391006496832911.\n",
    "\n",
    "The accuracy of the best model is 74% and precision is 79%. Therefore, we could not find a parameter combination that improves precision."
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Classification report\n",
    "y_probs = rs_logreg.best_estimator_.predict_proba(X_test_trans)[:, 1]\n",
    "\n",
    "y_pred = y_probs > 0.9\n",
    "print(f\"--- CLASSIFICATION REPORT {'-'*10}\")\n",
    "print(classification_report(y_test,y_pred))\n",
    "print(f\"--- CONFUSION MATRIX {'-'*10}\")\n",
    "print(confusion_matrix(y_test,y_pred))"
   ]
  },
  {
   "source": [
    "Unsurprisingly, changing the threshold value gives the same results as before."
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "source": [
    "## 7.2 KNN"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "source": [
    "### Data Transformation\n",
    "KNN compares observations based on a similarity measure. Therefore, we need to scale numerical features and use one-hot-encoding for our categorical features. Using one-hot encoding creates a sparse matrix and reduces KNN efficiency. Therefore, we remove category_name from our features to reduce the number of features."
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train_trans = X_train.copy()\n",
    "X_train_trans.pop(\"category_name\")\n",
    "X_test_trans = X_test.copy()\n",
    "X_test_trans.pop(\"category_name\")\n",
    "\n",
    "# Define transformer\n",
    "transformer = ColumnTransformer([\n",
    "    (\"scale\", ss, num_cols),\n",
    "    (\"encode\", onehot, make_column_selector(dtype_include=\"category\")),\n",
    "])\n",
    "\n",
    "# Transform\n",
    "X_train_trans = transformer.fit_transform(X_train)\n",
    "X_test_trans = transformer.transform(X_test)"
   ]
  },
  {
   "source": [
    "### 7.2.1 Simple KNN\n",
    "We will use the manhattan distance for similarity as our data is sparse."
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define Classifier\n",
    "knn = KNeighborsClassifier(p=1, n_jobs=-1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pred_eval_plot_model(X_train_trans , X_test_trans, y_train, y_test, knn)"
   ]
  },
  {
   "source": [
    "The model achieves a precision of 75% for successful projects. Training the KNN took very long and did not achieve large differences in precision. Therefore, we will not optimize KNN parameters with a grid saerch."
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "source": [
    "## 7.3 Decision Tree\n",
    "\n",
    "### Data Transformation\n",
    "\n",
    "For Decision Trees numerical data doesn't need to be scaled. Cateegorical data needs to be encoded. As One-Hot-Encoding leads to sparse data and decreases the performance of decision trees, we encode the categories numerically."
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Encode categorical features (with more than two classes)\n",
    "X_train_trans = X_train.copy()\n",
    "X_test_trans = X_test.copy()\n",
    "for cat in [\"country\", \"parent_category_name\", \"category_name\"]:\n",
    "    X_train_trans[[cat]] = X_train_trans[cat].cat.codes\n",
    "    X_test_trans[[cat]] = X_test_trans[cat].cat.codes"
   ]
  },
  {
   "source": [
    "### 7.3.1 Simple Decision Tree"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define Classifier\n",
    "dtree = DecisionTreeClassifier(random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "m_dtree = pred_eval_plot_model(X_train_trans, X_test_trans, y_train, y_test, dtree)"
   ]
  },
  {
   "source": [
    "The accuracy of 71% and precision of 73% for being successful needs to be improved further. Let's do a grid search:"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "source": [
    "### 7.3.2 Decion Tree with grid search"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "params_grid = {\n",
    "    \"max_depth\": np.arange(10, 50, 2),\n",
    "    \"min_samples_leaf\": np.arange(10, 30, 2),\n",
    "}\n",
    "rs_dtree = run_rand_grid_search(X_train_trans, X_test_trans, y_train, y_test, dtree, params_grid, n_iter=30)"
   ]
  },
  {
   "source": [
    "The currently best DecisionTree can be trained with {'min_samples_leaf': 33, 'max_depth': 28}. The model has an accuracy of 74% and precision of 76%."
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# export the decision tree to a tree.dot file \n",
    "# for visualizing the plot easily anywhere \n",
    "export_graphviz(rs_dtree.best_estimator_, out_file ='tree.dot')"
   ]
  },
  {
   "source": [
    "## 7.4 Random Forest"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "source": [
    "### Data Transformation\n",
    "For Random Forests we use the same data scaling and encoding as for decision trees."
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Encode categorical features (with more than two classes)\n",
    "X_train_trans = X_train.copy()\n",
    "X_test_trans = X_test.copy()\n",
    "for cat in [\"country\", \"parent_category_name\", \"category_name\"]:\n",
    "    X_train_trans[[cat]] = X_train_trans[cat].cat.codes\n",
    "    X_test_trans[[cat]] = X_test_trans[cat].cat.codes"
   ]
  },
  {
   "source": [
    "### 7.4.1 Simple Random Forest"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define Classifier\n",
    "rf = RandomForestClassifier()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "m_rf = pred_eval_plot_model(X_train_trans, X_test_trans, y_train, y_test, rf)"
   ]
  },
  {
   "source": [
    "Random Forest Classifier with default parameters has an accuracy of 75% and precision of 76%."
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate some stats for the random forest:\n",
    "n_nodes = []\n",
    "max_depths = []\n",
    "\n",
    "# Stats about the trees in random forest\n",
    "for ind_tree in m_rf.estimators_:\n",
    "    n_nodes.append(ind_tree.tree_.node_count)\n",
    "    max_depths.append(ind_tree.tree_.max_depth)\n",
    "    \n",
    "print(f'Average number of nodes {int(np.mean(n_nodes))}')\n",
    "print(f'Average maximum depth {int(np.mean(max_depths))}')"
   ]
  },
  {
   "source": [
    "### 7.4.2 Grid Search on random forest"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "params_grid = {\n",
    "    \"criterion\": [\"gini\", \"entropy\"],\n",
    "}\n",
    "rs_rf = run_grid_search(X_train_trans, X_test_trans, y_train, y_test, rf,  params_grid)"
   ]
  },
  {
   "source": [
    "According to the Grid Searc, entropy is the better criterion to select features and split-values, but accuracy and precision could not be improved."
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "source": [
    "## 7.5 ExtraTree"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "source": [
    "### Data Transformation\n",
    "Still working with trees, we keep the same data transformation:"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train_trans = X_train.copy()\n",
    "X_test_trans = X_test.copy()\n",
    "for cat in [\"country\", \"parent_category_name\", \"category_name\"]:\n",
    "    X_train_trans[[cat]] = X_train_trans[cat].cat.codes\n",
    "    X_test_trans[[cat]] = X_test_trans[cat].cat.codes"
   ]
  },
  {
   "source": [
    "### 7.5.1 Simple ExtraTreeClassifier"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "etree = ExtraTreesClassifier()\n",
    "m_etree = pred_eval_plot_model(X_train_trans, X_test_trans, y_train, y_test, etree)"
   ]
  },
  {
   "source": [
    "ExtraTreeClassifier has an accuracy of 74% and precision of 77% out of the box."
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "source": [
    "## 7.6 XGBoost"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "source": [
    "### Data Transformation\n",
    "We scale numerical features and encode categorical features with one-hot-encoding."
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define transformer\n",
    "transformer = ColumnTransformer([\n",
    "    (\"scale\", ss, num_cols),\n",
    "    (\"encode\", onehot, cat_cols),\n",
    "])\n",
    "\n",
    "# Transform\n",
    "X_train_trans = transformer.fit_transform(X_train)\n",
    "X_test_trans = transformer.transform(X_test)"
   ]
  },
  {
   "source": [
    "### 7.6.1 Simple XGB Classifier"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define Classifier\n",
    "xgb = XGBClassifier()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pred_eval_plot_model(X_train_trans, X_test_trans, y_train, y_test, xgb)"
   ]
  },
  {
   "source": [
    "XGBoost has an accuracy of 77% and precision of 79%, showing the best prediction results so far."
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "source": [
    "# 8. Future work"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "source": [
    "### Data Cleaning\n",
    "\n",
    "* Remove duplicates (based on id)\n",
    "* Extract more detailed location information (?)\n",
    "\n",
    "### Modeling\n",
    "\n",
    "* Stacked model (?)"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "horizontal-sentence",
   "metadata": {},
   "source": [
    "# 5. Prepare data for model training \n",
    "## 5.1 Define target and features:"
   ]
  },
  {
   "source": [],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "breathing-salem",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Select the target\n",
    "y = df.state\n",
    "\n",
    "# Select the features\n",
    "# aus KS_simple:\n",
    "# features = [\"backers_count\", \"converted_pledged_amount\" , \"goal\", \"disable_communication\", \"country\", \"staff_pick\", \"duration\"]\n",
    "\n",
    "# aus KS_model\n",
    "features = [\"backers_count\", \"converted_pledged_amount\" , \"goal\", \"disable_communication\", \"country_trans\", \"staff_pick\", \"duration\", \"prep_time\"]\n",
    "X = df[features]"
   ]
  },
  {
   "source": [
    "## Spliting the data in test and train\n",
    "\n",
    "Using the train_test_split method, with a 70/30 ratio, random_state = 42 and stratify = target (the latter shall secure that the propration of vales in the training and test set  have the same propration)"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "civilian-battlefield",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size = 0.3, random_state = 42, stratify = y)"
   ]
  },
  {
   "source": [
    "## Scaling and getting dummies for some models:"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Dummies\n",
    "X_train_dummies = pd.get_dummies(X_train, drop_first = True)\n",
    "X_test_dummies = pd.get_dummies(X_test, drop_first = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Scaling\n",
    "mms = MinMaxScaler()\n",
    "X_train_mms = mms.fit_transform(X_train)\n",
    "X_test_mms = mms.transform(X_test)"
   ]
  },
  {
   "source": [
    "# X. Creation, testing and comparing different models"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "source": [
    "## Define function to calculate the model metrics and ploting the confusion matrix:\n",
    "\n",
    "The function takes the splitet train, test data  and the model classifier.\n",
    "Calculate the model predictions, print out the classification_report with the model metrics and the confusion matrix, the latter it also plots."
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def pred_eval_plot_model (X_train, X_test, y_train, y_test, clf):\n",
    "    model = clf.fit(X_train, y_train) \n",
    "    \n",
    "    # Training predictions \n",
    "    y_train_pred = model.predict(X_train)\n",
    "    \n",
    "    # Testing predictions (to determine performance)\n",
    "    y_pred = model.predict(X_test)\n",
    "\n",
    "    print(classification_report(y_test,y_pred))\n",
    "    print(confusion_matrix(y_test,y_pred))\n",
    "    plot_confusion_matrix(model, X_test, y_test)\n",
    "    "
   ]
  },
  {
   "source": [
    "## X.1 Decision tree"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dtree = DecisionTreeClassifier(random_state = 42)\n",
    "pred_eval_plot_model(X_train, X_test, y_train, y_test, dtree)"
   ]
  },
  {
   "source": [
    "## 4.2 Random forest"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rf = RandomForestClassifier(random_state = 42)\n",
    "pred_eval_plot_model(X_train, X_test, y_train, y_test, rf)"
   ]
  },
  {
   "source": [
    "## 4.3 KNN\n",
    "\n",
    "Using the default numbers of neighbors (K = 5)"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "knn = KNeighborsClassifier()\n",
    "pred_eval_plot_model(X_train_dummies , X_test_dummies, y_train, y_test, knn)"
   ]
  },
  {
   "source": [
    "## 4.4 XGBoost"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "xgb = XGBClassifier()\n",
    "pred_eval_plot_model(X_train_dummies, X_test_dummies, y_train, y_test, xgb)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.8.5 64-bit ('.venv')",
   "language": "python",
   "name": "python38564bitvenv1b6b7a7d9e4f4a8683fe8e968060d28d"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5-final"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}